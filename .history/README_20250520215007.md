# Laser

<p align="center">
  :hugs: <a href="https://huggingface.co/collections/hkust-nlp/laser-682c7d44f347ac572ec054d3">HF Repo</a>&nbsp;&nbsp;&nbsp;
  :page_with_curl: <a href="">Paper</a>
</p>

This repo contains the resources for the paper "Laser: Learn to Reason Efficiently with Adaptive Length-based Reward Shaping"

Laser (**L**ength-b**A**sed **S**t**E**p **R**eward shaping) and its adaptive versions Laser-D, Laser-DE ( **Dynamic** and **D**ifficulty-aware **L**ength-b**A**sed **S**t**E**p **R**eward shaping) are three novel methods to successfully improve both the effectiveness and efficiency of reasoning. Laser-D and Laser-DE achieve a **6.1** improvement on AIME2024 while reducing token usage by **63\%**.

<p align="center">
  <img src="assets/main_figure.png" alt="Laser main figure">
</p> 

## Table of Contents

- [Laser](#laser)
  - [Table of Contents](#table-of-contents)
  - [News](#news)
  - [Introduction](#introduction)
    - [Unified Framework for Length-based Reward Shaping](#unified-framework-for-length-based-reward-shaping)
  - [Performance](#performance)
    - [Main Results](#main-results)
    - [Effectiveness of Adaptively Adjusting Exploration](#effectiveness-of-adaptively-adjusting-exploration)
  - [:rocket: M-STAR Resources](#rocket-m-star-resources)
  - [Citation](#citation)



## News

- :fire: [05/2025] We are excited to release the resources for the paper "Laser: Learn to Reason Efficiently with Adaptive Length-based Reward Shaping"

## Introduction

### Unified Framework for Length-based Reward Shaping


## Performance

### Main Results

<div align="center">

|                            | MathVista | FQA   | GPS   | MWP   | TQA   | VQA   |
|----------------------------|-----------|-------|-------|-------|-------|-------|
| **Baselines**              |           |       |       |       |       |       |
| MiniCPM-V-2.5              | 52.4      | 59.2  | 44.7  | 50.5  | 53.8  | 48.0  |
| &nbsp;&nbsp;&nbsp;+ warmup | 52.6      | 58.4  | 47.1  | 57.0  | 53.8  | 45.8  |
| SFT                        | 54.8      | 58.7  | 50.5  | 56.5  | 55.7  | 50.8  |
| ReST<sup>EM</sup>          | 55.1      | 59.1  | 49.5  | 65.6  | 55.1  | 48.0  |
| Iterative RFT              | 55.7      | 59.1  | 49.5  | 64.5  | 55.1  | 47.5  |
| **Static components only** |           |       |       |       |       |       |
| Cont. Self-Evolving        | 57.2      | 57.6  | 56.3  | 65.1  | 57.0  | 49.7  |
| &nbsp;&nbsp;&nbsp;+ PRM Re-Rank | 59.2  | 59.1↑0.7 | 61.1↑14 | 68.3↑11.3 | 55.1↑1.3 | 51.4↑5.6 |
| **Automatically tuning the temperature T** |   |       |       |       |       |       |
| M-STAR (Reward-Pass@2)     | 59.5 (+6.9) | 59.5↑1.1 | 59.1↑12 | 65.6↑8.6 | 58.9↑5.1 | 54.2↑8.4 |
| **Reference**              |           |       |       |       |       |       |
| GPT-4o                     | 63.8      |   -    |   -    |   -    |   -    |    -   |
| Gemini 1.5 Flash           | 58.4      |   -    |   -    |   -    |   -    |    -   |
| GPT-4T 2024-04-09          | 58.1      |   -    |   -    |   -    |   -    |    -   |
| Pixtral 12B                | 58.0      |   -    |   -    |   -    |   -    |    -   |
| InternLM-XComposer2-VL-7B  | 57.6      |   55.0	|  63.0	|  73.7	|  56.3	|  39.7	|
| Math-LLaVA-13B             | 46.6      |   37.2	| 57.7	| 56.5	| 51.3	| 33.5	|
| LLaVA-NeXT-34B             | 46.5      |   -    |   -    |   -    |   -    |    -   |

</div>


### Effectiveness of Adaptively Adjusting Exploration

<p align="center">
  <img src="./assets/dynamic.png" width="500">
</p>

Evaluating the effectiveness of adaptively adjusting exploration:

- **Reward-Pass@2**: The percentage of samples for which there exist correct responses among the top 2 responses ranked by the reward model. This metric directly reflects the exploitation efficacy of the reward model for the current policy. We choose Pass@2 since our training strategy involves selecting the top 2 responses using the reward model.

"Static" refers to models trained without adaptive exploration, while "Dynamic" indicates those trained with this mechanism. All models shown were trained using the M-STAR framework with optimized components as explored in our paper.

## :rocket: M-STAR Resources
<div align="center">

| Resource                                       | Link     | License  |
|------------------------------------------------|-----------|------------|
| **M-STAR Datasets**                          
| **M-STAR CoT Dataset**                        | [Link](https://huggingface.co/collections/hkust-nlp/m-star-676bbf9f749dbf511e7c4a32)       | [MIT License](https://opensource.org/license/mit)
| **M-STAR MPRM Training Dataset**              | [Link](https://huggingface.co/collections/hkust-nlp/m-star-676bbf9f749dbf511e7c4a32)       | [MIT License](https://opensource.org/license/mit)
| **M-STAR Models**                                   |           |             |
| M-STAR-8B-v1.0                |  [Link](https://huggingface.co/hkust-nlp/mstar-8b-v1.0)         | [MiniCPM Model License](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md)             |
| M-STAR-PRM-8B-v1.0               |  [Link](https://huggingface.co/hkust-nlp/mstar-prm-8b-v1.0)      | [MiniCPM Model License](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md)             |
</div>


## Citation
If you find the content of this project helpful, please cite our paper as follows:

```
...
```