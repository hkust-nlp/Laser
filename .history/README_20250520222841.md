# Laser

<p align="center">
  :hugs: <a href="https://huggingface.co/collections/hkust-nlp/laser-682c7d44f347ac572ec054d3">HF Repo</a>&nbsp;&nbsp;&nbsp;
  :page_with_curl: <a href="">Paper</a>
</p>

This repo contains the resources (**Code**, **Data**, **Models**) for the paper "Laser: Learn to Reason Efficiently with Adaptive Length-based Reward Shaping"

Laser (**L**ength-b**A**sed **S**t**E**p **R**eward shaping) and its adaptive versions Laser-D, Laser-DE ( **Dynamic** and **D**ifficulty-aware **L**ength-b**A**sed **S**t**E**p **R**eward shaping) are three novel methods to successfully improve both the effectiveness and efficiency of reasoning. Laser-D and Laser-DE achieve a **6.1** improvement on AIME2024 while reducing token usage by **63\%**.

<p align="center">
  <img src="assets/main_figure.png" alt="Laser main figure">
</p> 

## Table of Contents

- [Laser](#laser)
  - [Table of Contents](#table-of-contents)
  - [News](#news)
  - [Introduction](#introduction)
    - [Unified Framework for Length-based Reward Shaping](#unified-framework-for-length-based-reward-shaping)
  - [Performance](#performance)
    - [Efficacy-Efficiency Trade-off](#efficacy-efficiency-trade-off)
  - [:rocket: M-STAR Resources](#rocket-m-star-resources)
  - [Citation](#citation)


## News

- :fire: [05/2025] We are excited to release the resources for the paper "Laser: Learn to Reason Efficiently with Adaptive Length-based Reward Shaping"

## Introduction

In Laser, we propose a **unified view** for length-based reward shaping, unifying various reward-shaping and truncation methods. Building on this view, we propose a novel **L**ength-b**A**sed **S**t**E**p **R**eward shaping method (**Laser**), which employs a step reward function based on target length. We further propose the adaptive version of Laser, **Laser-D** and **Laser-DE**, based on two key intuitions: 

1. The reasoning behavior of the model evolves dynamically during training, necessitating reward specifications that are also adaptive and dynamic; 

2. Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. 

This approach facilitates a combination of fast and slow thinking, leading to a better overall tradeoff. Unlike methods that improve token efficiency at the expense of accuracy, our proposed approaches deliver substantial gains in both dimensionsâ€”even on the challenging AIME2024 benchmark.

### Unified Framework for Length-based Reward Shaping

We propose a unified framework for length-based reward shaping, unifying various reward-shaping and truncation methods. More details can be found in our [paper](), section 4.

<p align="center">
  <img src="assets/unified_framework.png" alt="Unified Framework for Length-based Reward Shaping">
</p>


## Performance

### Efficacy-Efficiency Trade-off
Efficacy (accuracy) and efficiency (token efficiency) are actually two conflicting goals. The goal of RL-based CoT compression should be to find a better balance between the two and improve both.

Each point in the following figures represents an independent experiment, obtained through different training runs with different parameter configurations.

<p align="center">
  <img src="assets/average_performance.jpg" alt="Average Performance" width="48%">
  <img src="assets/average_aime.jpg" alt="Average AIME Performance" width="48%">
</p>



## :rocket: M-STAR Resources
<div align="center">

| Resource                                       | Link     | License  |
|------------------------------------------------|-----------|------------|
| **M-STAR Datasets**                          
| **M-STAR CoT Dataset**                        | [Link](https://huggingface.co/collections/hkust-nlp/m-star-676bbf9f749dbf511e7c4a32)       | [MIT License](https://opensource.org/license/mit)
| **M-STAR MPRM Training Dataset**              | [Link](https://huggingface.co/collections/hkust-nlp/m-star-676bbf9f749dbf511e7c4a32)       | [MIT License](https://opensource.org/license/mit)
| **M-STAR Models**                                   |           |             |
| M-STAR-8B-v1.0                |  [Link](https://huggingface.co/hkust-nlp/mstar-8b-v1.0)         | [MiniCPM Model License](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md)             |
| M-STAR-PRM-8B-v1.0               |  [Link](https://huggingface.co/hkust-nlp/mstar-prm-8b-v1.0)      | [MiniCPM Model License](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md)             |
</div>


## Citation
If you find the content of this project helpful, please cite our paper as follows:

```
...
```